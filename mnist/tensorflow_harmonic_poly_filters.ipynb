{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Dimensional Convolutional Filters Using Harmonic Polynomials\n",
    "\n",
    "We use the standard convolutional net for MNIST as outlined in [the convolutional tensorflow tutorial on MNIST](https://www.tensorflow.org/tutorials/estimators/cnn), except we restrict the dimensions of our filters using harmonic polynomials.\n",
    "\n",
    "## Filter Dimension Restriction\n",
    "\n",
    "The normal filter consists of a `5x5` patch for each input channel and output channel; i.e. a tensor of shape `(5, 5, n_input_channels, n_output_channels)`. Each `5x5` patch has dimension `25` when the coefficients of the patch are not restricted.\n",
    "\n",
    "Here we reduce the dimensions of each `5x5` patch to a lower-dimensional sub-space, in particular we look at restricting each\n",
    "`5x5` patch to have values in the space of harmonic polynomials of `x` and `y` with degree at most `3`. This is a `7`-dimesional space. This greatly reduces the number of trainable coefficients in the model.\n",
    "\n",
    "### Why Harmonic?\n",
    "\n",
    "We are interested in finding patches that are going to be good at picking out different types of edges. So they need a good\n",
    "mix of positive and negative values. That is, they need to avoid having maxima/minima such as `p(x,y) = x**2 + y**2`. We also make the basis orthonormal.\n",
    "\n",
    "Here is what the harmonic filter basis looks like:\n",
    "\n",
    "![Othonormal Harmonic Filter Basis](files/graphs/orthonormal_harmonic_polys.png)\n",
    "\n",
    "### How to Restrict Dimensions of Filters?\n",
    "\n",
    "To restrict the filters we start with a basis of filters of shape `(5, 5, n_basis)`. We use a non-trainable Depthwise Convolution2D layer to get coefficients coming from this filter sub-space. Now, a Depthwise Convoution2D layer actually needs\n",
    "filters for each input channel, so we actually make `n_input_channels` copies of the filter basis to make a tensor of shape\n",
    "`(5, 5, n_input_channels, n_basis)`. This is the depthwise filter for the Depthwise Convoutional layer. The output of this\n",
    "is of shape `(5, 5, n_basis * n_input_channels)`. The respective outputs of each input channel is grouped into contiguous\n",
    "segments of size `n_basis` along the last axis.\n",
    "\n",
    "To then train on the output of the (non-trainable) Depthwise Convolution2D Layer, we next add a regular Convolution2D layer with filter of shape `(1, 1, n_basis * n_output_channels, n_output_channels)`. This effectively creates a point-wise convolution over the coefficients from the restricted filter sub-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## The Effective Kernels\n",
    "\n",
    "### First Convolution\n",
    "\n",
    "Here are pictures of the effective kernels for the first 2D Convolution.\n",
    "\n",
    "![Effective Kernels for First Convolution](files/graphs/effective_filters_1.png)\n",
    "\n",
    "What do these filters do to the image inputs? Consider the following example image\n",
    "\n",
    "![Example Digit Image](files/graphs/example_orig.svg)\n",
    "\n",
    "Here are the graphs of the output of the first (effective) 2D Convolution with combined 2D Pooling:\n",
    "\n",
    "![Output of First Effective 2D Convolution](files/graphs/conv_pool1.png)\n",
    "\n",
    "### Second Convolution\n",
    "\n",
    "Here are the effective kernels for the second 2D Convolution.\n",
    "\n",
    "![Effective Kernels for Second Convolution](files/graphs/effective_filters_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Data Files\n",
    "\n",
    "The dataset is from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/). The decription of the file formats\n",
    "is also given there.\n",
    "\n",
    "## Image File Format\n",
    "```\n",
    "[offset] [type]          [description] \n",
    "0000     32 bit integer  magic number \n",
    "0004     32 bit integer  number of images \n",
    "0008     32 bit integer  number of rows \n",
    "0012     32 bit integer  number of columns \n",
    "0016     unsigned byte   pixel \n",
    "0017     unsigned byte   pixel \n",
    "........ \n",
    "xxxx     unsigned byte    pixel\n",
    "```\n",
    "\n",
    "## Label File Format\n",
    "```\n",
    "[offset] [type]          [description] \n",
    "0000     32 bit integer  magic number (MSB first) \n",
    "0004     32 bit integer  number of items \n",
    "0008     unsigned byte   label \n",
    "0009     unsigned byte   label \n",
    "........ \n",
    "xxxx     unsigned byte   label\n",
    "The labels values are 0 to 9.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct # To unpack string literals of bytes to integers.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none' # Saves space when saving svg plots to file.\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {}\n",
    "y = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training images information.\n",
    "\n",
    "with open('data/train-images.idx3-ubyte', 'rb') as f:\n",
    "    _ = f.read(4) # Read the magic number.\n",
    "    training_info = {name : f.read(4) for name in ['n_images', 'n_rows', 'n_columns']}\n",
    "    print(training_info)\n",
    "    # Make sure to enforce big-endian.\n",
    "    training_info = {key : struct.unpack('>i', value)[0] for key, value in training_info.items()}\n",
    "    print(training_info)\n",
    "    \n",
    "    images = np.fromfile(f, dtype = 'uint8')\n",
    "    images = images.reshape(training_info['n_images'], training_info['n_rows'], training_info['n_columns'])\n",
    "X['train'] = images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training labels.\n",
    "\n",
    "with open('data/train-labels.idx1-ubyte', 'rb') as f:\n",
    "    _ = f.read(4) # Read the magic number.\n",
    "    training_info['n_labels'] = struct.unpack('>i', f.read(4))[0]\n",
    "    print(training_info)\n",
    "        \n",
    "    labels = np.fromfile(f, dtype = 'uint8')\n",
    "y['train'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_i = 3\n",
    "plt.imshow(images[sample_i])\n",
    "plt.title('Label = ' + str(y['train'][sample_i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y['train'], return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test images.\n",
    "\n",
    "with open('data/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "    _ = f.read(4) # Read the magic number.\n",
    "    testing_info = {name : f.read(4) for name in ['n_images', 'n_rows', 'n_columns']}\n",
    "    print(testing_info)\n",
    "    # Make sure to enforce big-endian.\n",
    "    testing_info = {key : struct.unpack('>i', value)[0] for key, value in testing_info.items()}\n",
    "    print(testing_info)\n",
    "    \n",
    "    images = np.fromfile(f, dtype = 'uint8')\n",
    "    images = images.reshape(testing_info['n_images'], testing_info['n_rows'], testing_info['n_columns'])\n",
    "X['test'] = images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training labels.\n",
    "\n",
    "with open('data/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "    _ = f.read(4) # Read the magic number.\n",
    "    testing_info['n_lables'] = struct.unpack('>i', f.read(4))[0]\n",
    "    print(testing_info)\n",
    "        \n",
    "    labels = np.fromfile(f, dtype = 'uint8')\n",
    "y['test'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the First Convolution Filter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_orthonormal(filter_basis):\n",
    "    filter_basis = filter_basis.T.astype('float32')\n",
    "    print(filter_basis.shape)\n",
    "    for i, channel in enumerate(filter_basis):\n",
    "        for normalized in filter_basis[:i]:\n",
    "            dot = (channel * normalized).sum()\n",
    "            channel = channel - dot * normalized\n",
    "        filter_basis[i] = channel / np.linalg.norm(channel)\n",
    "    return filter_basis.T\n",
    "\n",
    "def make_filter_basis(shape_2d, fncs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    fncs : List of Functions\n",
    "        The funcs f(x,y) to use to make the filter basis.\n",
    "    '''\n",
    "    n_channels = len(fncs)\n",
    "    x_coord = np.full(shape_2d, np.arange(shape_2d[1]))\n",
    "    y_coord = np.full((shape_2d[1], shape_2d[0]), np.arange(shape_2d[0])).T\n",
    "    filter_base = np.array([x_coord, y_coord])\n",
    "    \n",
    "    basis = [f(filter_base) for f in fncs]\n",
    "    basis = [x / np.linalg.norm(x) for x in basis]\n",
    "    basis = np.array(basis).T.astype('float32')\n",
    "    \n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get harmonic polynomials.\n",
    "# Use default-value for function currying inside list expressions.\n",
    "\n",
    "max_degree = 3\n",
    "fncs =  ([[lambda X : X[0]**0]] +\n",
    "        [[lambda X, p = i : np.real((X[0] - 2 + 1j * (X[1] - 2))**p),\n",
    "         lambda X, p = i : np.imag((X[0] - 2 + 1j * (X[1] - 2))**p)]\n",
    "            for i in np.arange(1, max_degree + 1, 1)])\n",
    "\n",
    "#Flatter the list of functions.\n",
    "fncs = [a for inner in fncs\n",
    "          for a in inner]\n",
    "\n",
    "filter_basis_harmonic = make_filter_basis((5, 5), fncs)\n",
    "print(filter_basis_harmonic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(filter_basis_harmonic.shape[-1]):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(filter_basis_harmonic[:, :, i])\n",
    "    plt.title('Filter ' + str(i))\n",
    "    plt.xticks([0, 2, 4])\n",
    "    plt.yticks([0, 2, 4])\n",
    "    ax = plt.gca()\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/harmonic_polys.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_basis_orthonormal = make_orthonormal(filter_basis_harmonic)\n",
    "for i in range(filter_basis_orthonormal.shape[-1]):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(filter_basis_orthonormal[:, :, i])\n",
    "    plt.title('Filter ' + str(i))\n",
    "    plt.xticks([0, 2, 4])\n",
    "    plt.yticks([0, 2, 4])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/orthonormal_harmonic_polys.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simple Harmonic Polynomial Filter Basis\n",
    "\n",
    "![Simple Harmonic Polynomial Filters](graphs/harmonic_polys.png)\n",
    "\n",
    "## The Orthonormal Harmonic Polynomial Filter Basis\n",
    "\n",
    "![Orthonormal Harmonic Polynomial Filters](graphs/orthonormal_harmonic_polys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "\n",
    "The different equiaffine maps give us different channels for each image. Then we use 3d convolution to make sure that we apply\n",
    "the same filter to each equiaffine map result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make layers for low-dimensional convolution.\n",
    "\n",
    "def make_low_dim_conv2d_layers(filter_basis, n_input_channels, n_filters, name):\n",
    "    '''\n",
    "    filter_basis : np.nd_array\n",
    "        Has shape (filter_height, filter_width, n_basis)\n",
    "    '''\n",
    "    # Add axis for channels. Depthwise Conv2D will need a copy for each channel.\n",
    "    filter_basis = filter_basis[..., np.newaxis, :]\n",
    "    print('filter_basis.shape = ', filter_basis.shape)\n",
    "    \n",
    "    depthwise_filter = np.full(filter_basis.shape[:-2] + (n_input_channels,) + filter_basis.shape[-1:], filter_basis)\n",
    "    print('depthwise_filter.shape = ', depthwise_filter.shape)\n",
    "    depthwise_filter_init = tf.keras.initializers.Constant(depthwise_filter)\n",
    "    layers = [tf.keras.layers.DepthwiseConv2D(kernel_size = filter_basis.shape[:2],\n",
    "                                              depth_multiplier = filter_basis.shape[-1],\n",
    "                                              padding = 'same',\n",
    "                                              use_bias = False,\n",
    "                                              depthwise_initializer = depthwise_filter_init,\n",
    "                                              trainable = False,\n",
    "                                              name = name + '_filter_space'),\n",
    "             tf.keras.layers.Conv2D(kernel_size = (1, 1),\n",
    "                                    padding = 'valid',\n",
    "                                    filters = n_filters,\n",
    "                                    activation = tf.nn.relu,\n",
    "                                    name = name + '_pointwise_conv2D')]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct layers of the complete model.\n",
    "\n",
    "def make_layers(filter_basis, n_filters, input_shape):\n",
    "    \n",
    "    layers = [tf.keras.layers.Reshape(input_shape = input_shape,\n",
    "                                      target_shape = input_shape + (1,),\n",
    "                                      name = 'Initial_Make_Channel')]\n",
    "          \n",
    "    layers += make_low_dim_conv2d_layers(filter_basis[0], 1, n_filters[0], 'low_dim_conv2d_1')\n",
    "\n",
    "    layers += [tf.keras.layers.MaxPool2D(pool_size = (2, 2),\n",
    "                                         strides = (2, 2),\n",
    "                                         name = 'MaxPool_1')]\n",
    "\n",
    "    layers += make_low_dim_conv2d_layers(filter_basis[1], n_filters[0], n_filters[1], 'low_dim_conv2d_2')\n",
    "\n",
    "    layers += [tf.keras.layers.MaxPool2D(pool_size = (2, 2),\n",
    "                                         strides = (2, 2),\n",
    "                                         name = 'MaxPool_2'),\n",
    "               tf.keras.layers.Reshape(target_shape = (n_filters[1] * (input_shape[0]//4) * (input_shape[1]//4),),\n",
    "                                       name = 'Reshape_to_1D'),\n",
    "               tf.keras.layers.Dense(units = (n_filters[1] * (input_shape[0]//4) * (input_shape[1]//4)) // 3,\n",
    "                                     activation = tf.nn.relu,\n",
    "                                     name = 'Dense_1'),\n",
    "               tf.keras.layers.Dropout(rate = 0.4),\n",
    "               tf.keras.layers.Dense(units = 10,\n",
    "                                     activation = tf.nn.softmax,\n",
    "                                     name = 'Class_Logits')\n",
    "         ]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_layers = make_layers(filter_basis = [filter_basis_orthonormal for _ in range(2)],\n",
    "                              n_filters = [32, 64],\n",
    "                              input_shape = X['train'].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'harmonic_poly_model.h5'\n",
    "try:\n",
    "    harmonic_model = tf.keras.models.load_model('saved_models/' + filename)\n",
    "    print('Model automatically LOADED from file ' + filename)\n",
    "except:\n",
    "    print('File saved_models/' + filename + ' can\\'t be opened. Rebuilding and retraining model.')\n",
    "    harmonic_model = tf.keras.Sequential(harmonic_layers)\n",
    "    harmonic_model.compile(optimizer = 'adam',\n",
    "                           loss='sparse_categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "    harmonic_model.fit(X['train'], y['train'], epochs = 5)\n",
    "    print('Model saved to saved_models/' + filename)\n",
    "    harmonic_model.save('saved_models/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = harmonic_model.evaluate(X['test'], y['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Final Kernels for Harmonic Polynomial Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the depthwise-kernel from the model with the pointwise-kernel to get the effective\n",
    "# kernels for the first 2D Convolution.\n",
    "\n",
    "depthwise_kernel = tf.keras.backend.eval(harmonic_model.layers[1].depthwise_kernel)\n",
    "print('depthwise_kernel.shape = ', depthwise_kernel.shape)\n",
    "pointwise_kernel = tf.keras.backend.eval(harmonic_model.layers[2].kernel)\n",
    "print('pointwise_kernel.shape = ', pointwise_kernel.shape)\n",
    "effective_kernel = np.dot(depthwise_kernel, pointwise_kernel[0, 0, ...])\n",
    "print('effective_kernel.shape = ', effective_kernel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the effective kernels for the first 2D convolution.\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i in range(32):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    plt.imshow(effective_kernel[:, :, 0, i])\n",
    "    if i != 3 and i != 4:\n",
    "        plt.title('Filter ' + str(i))\n",
    "    if i != 0:\n",
    "        plt.xticks([0, 2, 4])\n",
    "        plt.yticks([0, 2, 4])\n",
    "\n",
    "plt.suptitle('Effective Filters for First Convolution')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/effective_filters_1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picture of the Effective Filters for the First Convolution\n",
    "\n",
    "![The effective kernels of the first 2D Convolution](files/graphs/effective_filters_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get teh effective kernels for the second convolution.\n",
    "\n",
    "depthwise_2 = tf.keras.backend.eval(harmonic_model.layers[4].depthwise_kernel)\n",
    "print('depthwise_2.shape = ', depthwise_2.shape)\n",
    "pointwise_2 = tf.keras.backend.eval(harmonic_model.layers[5].kernel)\n",
    "print('pointwise_2.shape = ', pointwise_2.shape)\n",
    "ind_pointwise = np.arange(pointwise_2.shape[2])\n",
    "depthwise_2 = depthwise_2[:, :, ind_pointwise // 7, ind_pointwise % 7]\n",
    "combination_2 = np.dot(depthwise_2, pointwise_2[0, 0, ...])\n",
    "print('combination_2.shape = ', combination_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the effective kernels for the second 2D convolution.\n",
    "\n",
    "fig = plt.figure(figsize = (15, 7))\n",
    "for i in range(64):\n",
    "    plt.subplot(5, 13, i + 1)\n",
    "    plt.imshow(combination_2[:, :, i])\n",
    "    if i < 5 or i > 7:\n",
    "        plt.title('Filter ' + str(i))\n",
    "    ax = plt.gca()\n",
    "    if i > 0:\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.xticks([0, 2, 4])\n",
    "        plt.yticks([0, 2, 4])\n",
    "\n",
    "plt.suptitle('Effective Kernels for Second 2D Convolution')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/effective_filters_2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picture of Effective Kernels For Second Convolution\n",
    "\n",
    "![Effective Kernels for Second 2D Convolution](files/graphs/effective_filters_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at Output of Convolution Layers for Particular Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the example.\n",
    "\n",
    "example = X['test'][20]\n",
    "plt.imshow(example)\n",
    "ax = plt.gca()\n",
    "plt.xticks(np.arange(0, 28, 10))\n",
    "plt.yticks(np.arange(0, 28, 10))\n",
    "plt.title('Original Example Input')\n",
    "plt.savefig('graphs/example_orig.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result of applying the first convolutional layers.\n",
    "\n",
    "result = example.reshape(1, 28, 28, 1)\n",
    "for i in range(4):\n",
    "    result = harmonic_model.layers[i](result)\n",
    "result = tf.keras.backend.eval(result)\n",
    "print('result.shape = ', result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the results of the first 2D convolution.\n",
    "\n",
    "fig = plt.figure(figsize = (15, 10))\n",
    "for i in range(32):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    plt.imshow(result[0, :, :, i])\n",
    "    ax = plt.gca()\n",
    "    \n",
    "plt.suptitle('Result of First 2D Convolution For Example')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/conv_pool1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Output for First Convolution and Pooling\n",
    "\n",
    "The original input is:\n",
    "\n",
    "![Original](files/graphs/example_orig.svg)\n",
    "\n",
    "The output of the first convolution and pooling layers is:\n",
    "\n",
    "![First Convolution and Pooling](files/graphs/conv_pool1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use LDA of 5x5 sub-samples to pick out filter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get 5x5 sub-samples from image data.\n",
    "\n",
    "def make_sub_samples(images):\n",
    "    images = images[:, 1:26, 1:26] # Drop extra\n",
    "    i_ind = 5 * np.arange(5)[..., np.newaxis, np.newaxis, np.newaxis] + np.arange(5)[..., np.newaxis]\n",
    "    j_ind = 5 * np.arange(5)[..., np.newaxis, np.newaxis] + np.arange(5)\n",
    "    sub_samples = images[:, i_ind, j_ind]\n",
    "    sub_samples = sub_samples.reshape(-1, 5, 5)\n",
    "    return sub_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sub-samples from the training data.\n",
    "\n",
    "sub_samples = make_sub_samples(X['train'])\n",
    "sub_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at a sub-sample for a particular image.\n",
    "\n",
    "plt.imshow(X['train'][0])\n",
    "plt.title('Original Image')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sub_samples[6])\n",
    "plt.title('Sub-sample 6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for the sub-samples.\n",
    "\n",
    "sub_sample_labels = [label for label in y['train']\n",
    "                           for _ in range(5 * 5)] # Make sure to repeat for sub-samples\n",
    "sub_sample_labels = np.array(sub_sample_labels)\n",
    "sub_sample_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA on sub-samples data.\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(sub_samples.reshape(-1, 25), sub_sample_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LDA to find a sub-space of filters for the first convolution.\n",
    "\n",
    "# For comparision purposes, use the same dimension as before.\n",
    "n_lda = 7 \n",
    "\n",
    "# Reshape the lda coefficients to get basis.\n",
    "filter_basis_lda = np.rollaxis(lda.coef_[:n_lda].reshape(-1, 5, 5), 0, 3) \n",
    "print(filter_basis_lda.shape)\n",
    "plt.imshow(filter_basis_lda[:, :, 0])\n",
    "plt.title('LDA Filter Basis Element 0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph all of the filters picked out by the LDA.\n",
    "\n",
    "for i in range(filter_basis_lda.shape[-1]):\n",
    "    plt.subplot(2, 4, i + 1)\n",
    "    plt.imshow(filter_basis_lda[:, :, i])\n",
    "    plt.title('Filter ' + str(i))\n",
    "\n",
    "plt.suptitle('LDA Filter Basis')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the layers for the model that uses the LDA filter basis for the first 2d convolution.\n",
    "\n",
    "lda_layers = make_layers(filter_basis = [filter_basis_lda, filter_basis_harmonic],\n",
    "                         n_filters = [32, 64],\n",
    "                         input_shape = X['train'].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'low_dim_filter_lda.h5'\n",
    "try:\n",
    "    lda_model = tf.keras.models.load_model('saved_models/' + filename)\n",
    "    print('Model automatically LOADED from file ' + filename)\n",
    "except:\n",
    "    print('File saved_models/' + filename + ' can\\'t be opened. Rebuilding and retraining model.')\n",
    "    lda_model = tf.keras.Sequential(lda_layers)\n",
    "    lda_model.compile(optimizer = 'adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    lda_model.fit(X['train'], y['train'], epochs = 5)\n",
    "    print('Model saved to saved_models/' + filename)\n",
    "    lda_model.save('saved_models/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = lda_model.evaluate(X['test'], y['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
