{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from my_src import my_graphs, my_periodic\n",
    "\n",
    "# When saving svg plots, we use regular text for plot text. This stops the default\n",
    "# behavior of outputting curves for each letter of text and saves a lot of memory\n",
    "# per graph.\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "violin_plot_grid_size = 20\n",
    "violin_plot_fig_size = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/uber-raw-data-aug14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date/Time'] = pd.to_datetime(data['Date/Time'])\n",
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_date = data['Date/Time'].apply(lambda x : (x.month, x.day, x.hour))\n",
    "reduced_date = pd.DataFrame(reduced_date.tolist(), columns = ['month', 'day', 'hour'])\n",
    "reduced_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get hourly counts by day and hour.\n",
    "\n",
    "hour_counts = (reduced_date.groupby(['day', 'hour'])\n",
    "                .count()\n",
    "                .rename(columns =  {'month' : 'count'}))\n",
    "hour_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_day = 4\n",
    "day_of_week = hour_counts.index.to_series().apply(lambda x : (x[0] - 1 + initial_day) % 7)\n",
    "print('Before adding in hourly part\\n', day_of_week.value_counts().sort_index())\n",
    "day_of_week += hour_counts.index.get_level_values('hour') / 24\n",
    "day_of_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(day_of_week, hour_counts['count'])\n",
    "plt.xlabel('Day of the Week (to the Hour)')\n",
    "plt.ylabel('Hourly Pickup Count')\n",
    "plt.title('Hourly Pickup Counts Through the Week')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/continuous_hourly_counts.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the hourly pickup counts for the different times of the week.\n",
    "\n",
    "![Hourly Pickup Counts vs Continuous Day of the Week](files/graphs/continuous_hourly_counts.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Statistics Using Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bin statistics.\n",
    "\n",
    "bin_size_hours = 2\n",
    "bin_size = 1 / 24 * bin_size_hours\n",
    "bins = np.arange(0, 7 + 1/24, bin_size)\n",
    "bin_cut = pd.cut(day_of_week, bins = bins)\n",
    "bin_mean = hour_counts.groupby(bin_cut).mean().sort_index()\n",
    "bin_variance = (hour_counts.groupby(bin_cut).std()**2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bin variance vs the bin mean. Use logarithmic scales.\n",
    "\n",
    "bin_model = LinearRegression()#fit_intercept = False)\n",
    "bin_model.fit(np.log10(bin_mean), np.log10(bin_variance).values.reshape(-1))\n",
    "print('log_bin_variance = ', '{:.3f}'.format(bin_model.coef_[0]), ' * log_bin_mean + ',\n",
    "      '{:.3f}'.format(bin_model.intercept_))\n",
    "\n",
    "line_x = np.linspace(np.log10(bin_mean.min()), np.log10(bin_mean.max()), 4)\n",
    "plt.scatter(np.log10(bin_mean), np.log10(bin_variance))\n",
    "plt.plot(line_x, bin_model.predict(line_x), color = 'red')\n",
    "plt.xlabel('Log10(Bin Mean)')\n",
    "plt.ylabel('Log10(Bin Variance)')\n",
    "plt.title('Comparison of Bin Variances to Bin Means')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/bin_var_bin_mean.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the logarithms of the bin variances to the logarithms of the bin means, we see that\n",
    "```\n",
    "log_bin_variance = 1.251 * log_bin_mean + 0.352\n",
    "```\n",
    "\n",
    "![Log10(Bin_Variance) vs Log10(Bin_Mean)](files/graphs/bin_var_bin_mean.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Periodic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the periods for weekly and daily trends.\n",
    "\n",
    "n_weekly_fourier = 30\n",
    "n_daily_fourier = 11 # For daily fourier, don't go over 11; the data is sampled by the hour.\n",
    "                     # Remember that dimension doubles and can't exceed 24. Also we are\n",
    "                     # already adding constant functions in addition to fourier stuff.\n",
    "\n",
    "weekly_frequency = np.arange(1, n_weekly_fourier + 1) # per week\n",
    "daily_frequency = 7 * np.arange(1, n_daily_fourier + 1) # per week\n",
    "all_frequency = np.concatenate([weekly_frequency, daily_frequency])\n",
    "all_frequency = np.unique(all_frequency)\n",
    "\n",
    "all_periods = 7.0 / all_frequency\n",
    "print(all_periods.shape)\n",
    "all_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the periodic features and the alphas for regularization.\n",
    "\n",
    "y = hour_counts['count']\n",
    "X, alpha_base = my_periodic.make_periodic(day_of_week, all_periods, return_alphas = 'L1')\n",
    "\n",
    "column_names = ['freq_' + str(freq) for freq in all_frequency]\n",
    "column_names = [[col + '_c', col + '_s'] for col in column_names]\n",
    "column_names = np.array(column_names).reshape(-1)\n",
    "X = pd.DataFrame(X, index = day_of_week.index, columns = column_names)\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Non-Regularized NegativeBinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = {'unregularized' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_nbinom_params(statsmodels_params, means, loglike_method):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      nb2:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    \n",
    "    theta = 1 / statsmodels_params['alpha']\n",
    "    if loglike_method == 'nb2':\n",
    "        ns = theta\n",
    "        ps = 1 / (1 + statsmodels_params['alpha'] * means)\n",
    "    elif loglike_method == 'nb1':\n",
    "        ns = theta * means\n",
    "        ps = theta / (1 + theta)\n",
    "    else:\n",
    "        raise Exception('loglike_method not a valid value.')\n",
    "    return {'n' : ns, 'p' : ps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function for computing log-likelihood of negative binomial model.\n",
    "\n",
    "def find_log10_likelihood(fitted_model, X, y, loglike_method, add_constant = True):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)   \n",
    "        \n",
    "    log_lls = sts.nbinom.logpmf(k = y, **nbinom_kwargs) / np.log(10)\n",
    "    return log_lls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbinomial_start_params(X, y, loglike_method, add_constant = True):\n",
    "    _, n_zeros = X.shape\n",
    "    if add_constant:\n",
    "        n_zeros = n_zeros + 1\n",
    "        \n",
    "    y_mean = y.mean()\n",
    "    if loglike_method == 'nb2':\n",
    "        initial_alpha = (y.std()**2 - y_mean) / y_mean**2\n",
    "    elif loglike_method == 'nb1':\n",
    "        initial_alpha = (y.std()**2 - y_mean) / y_mean\n",
    "    elif loglike_method != 'geometric':\n",
    "        raise Exception('loglike_method value not recognized.')\n",
    "        \n",
    "    if loglike_method != 'geometric':\n",
    "        start_params = np.zeros(n_zeros + 1)\n",
    "        start_params[-1] = initial_alpha\n",
    "    else:\n",
    "        start_params = np.zeros(n_zeros)\n",
    "        \n",
    "    start_params[0] = np.log(y_mean)\n",
    "    return start_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "start_params = get_nbinomial_start_params(X, y, loglike_method = 'nb1')\n",
    "\n",
    "best_scores['unregularized'] = {key : [] for key in ['train', 'test']}\n",
    "\n",
    "for split_ind in splitter.split(X):\n",
    "    keys = ['train', 'test']\n",
    "    X_split = {key : statsmodels.tools.add_constant(X.loc[ind, :], has_constant = 'add') for key, ind in zip(keys, split_ind)}\n",
    "    y_split = {key : y.loc[ind, :] for key, ind in zip(keys, split_ind)}\n",
    "    negative_binomial = NegativeBinomial(endog = y_split['train'], exog = X_split['train'])\n",
    "    fitted_model = negative_binomial.fit(start_params = start_params)\n",
    "    \n",
    "    for key in best_scores['unregularized']:\n",
    "            log_ll = find_log10_likelihood(fitted_model, X_split[key], y_split[key],\n",
    "                                           loglike_method = 'nb1').mean()\n",
    "            best_scores['unregularized'][key].append(log_ll)\n",
    "            \n",
    "for key in best_scores['unregularized']:\n",
    "    best_scores['unregularized'][key] = np.array(best_scores['unregularized'][key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['unregularized'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['unregularized'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for UnRegularized Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = fitted_model.predict(statsmodels.tools.add_constant(X))\n",
    "plt.plot(y_predict.values)\n",
    "plt.title('Example of Unregularized Fit')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Function for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gridsearch_nbinom(X, y, alphas, loglike_method, **fit_kwargs):\n",
    "    negative_binomial = NegativeBinomial(endog = y['train'],\n",
    "                                         exog = X['train'],\n",
    "                                         loglike_method = loglike_method)\n",
    "    search_scores = {key : [] for key in X}\n",
    "    start_params = get_nbinomial_start_params(X['train'], y['train'], loglike_method, add_constant = False)\n",
    "    \n",
    "    for alpha in alphas: \n",
    "        fitted_model = negative_binomial.fit_regularized(alpha = alpha, \n",
    "                                                         start_params = start_params,\n",
    "                                                         **fit_kwargs)\n",
    "        for key in search_scores:\n",
    "            log_ll = find_log10_likelihood(fitted_model, X[key], y[key],\n",
    "                                           loglike_method = loglike_method).mean()\n",
    "            search_scores[key].append(log_ll)\n",
    "        # Reset start_params to last fitted values for speed-up.\n",
    "        start_params = np.array(fitted_model.params)\n",
    "        \n",
    "    return search_scores\n",
    "    \n",
    "def do_cv_nbinom(X, y, log_alpha_sizes, base_alphas, loglike_method, splitter, add_constant = True, **fit_kwargs):\n",
    "    if add_constant:\n",
    "        base_alphas = np.concatenate([[0], # No penalty for bias term.\n",
    "                                      base_alphas, # Penalties for coefficients.\n",
    "                                      [0]]) # No penalty for alpha parameter related to latent\n",
    "                                            # Poisson distribution.\n",
    "        X = statsmodels.tools.add_constant(X, has_constant = 'add')\n",
    "    else:\n",
    "        base_alphas = np.concatenate([base_alphas, # Penalties for coefficients.\n",
    "                                     [0]]) # No penalty for alpha parameter related to latent\n",
    "                                           # Poisson distribution.\n",
    "    cv_scores = {'train' : [], 'test' : []}   \n",
    "    \n",
    "    for split_ind in splitter.split(X, y):\n",
    "        split_ind = {'train' : X.index[split_ind[0]],\n",
    "                     'test' : X.index[split_ind[1]]}\n",
    "        X_split = {key : X.loc[ind] for key, ind in split_ind.items()} \n",
    "        y_split = {key : y.loc[ind] for key, ind in split_ind.items()}\n",
    "        alphas = (base_alphas * np.exp(log_size) for log_size in log_alpha_sizes)\n",
    "        search_scores = do_gridsearch_nbinom(X_split, y_split, alphas, loglike_method, **fit_kwargs)\n",
    "        for key in cv_scores:\n",
    "            cv_scores[key].append(search_scores[key])\n",
    "        \n",
    "    for key in cv_scores:\n",
    "        cv_scores[key] = np.array(cv_scores[key])\n",
    "    return cv_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Regularized Fit With Flat Penalty Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_alpha = {'flat' : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "log_alpha_sizes = np.linspace(6, 8, 15)\n",
    "flat_alpha_base = np.full(alpha_base.shape, 1.0)\n",
    "flat_alpha_base = flat_alpha_base / np.linalg.norm(flat_alpha_base)\n",
    "cv_scores = do_cv_nbinom(X, y, log_alpha_sizes = log_alpha_sizes, base_alphas = flat_alpha_base, \n",
    "                         loglike_method = 'nb1', \n",
    "                         splitter = splitter,\n",
    "                         trim_mode = 'off',\n",
    "                         maxiter = 300,\n",
    "                         disp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('All Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].T, color = color)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Mean Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].mean(axis = 0), color = color)\n",
    "    \n",
    "plt.legend(cv_scores.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_alpha_sizes, cv_scores['test'].mean(axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cutoff = 7.25\n",
    "best_ind = (log_alpha_sizes < best_cutoff).sum() - 1\n",
    "best_log_alpha['flat'] = log_alpha_sizes[best_ind]\n",
    "best_scores['flat'] = {key : cv_scores[key][:, best_ind] for key in cv_scores}\n",
    "best_log_alpha['flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['flat'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['flat'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for Best Flat Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Regularized Negative Binomial Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to estimate risk between model distribution and real distribution.\n",
    "# Note we don't calculute the square term for the true distribution as it is\n",
    "# independent of the model.\n",
    "\n",
    "def find_risk(fitted_model, X, y, loglike_method, add_constant = True, n_draws_per_point = 50):\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)\n",
    "    \n",
    "    # Approximate the cross-term by turning integral in over true distribution into a sample\n",
    "    # mean of model probability.\n",
    "    \n",
    "    cross_term = sts.nbinom.pmf(k = y, **nbinom_kwargs).mean()\n",
    "    \n",
    "    # Approximate the square-term by making draws from the model distribution for each X_i.\n",
    "    kwargs_broadcast = {key : np.array(param).reshape(-1, 1) for key, param in nbinom_kwargs.items()}\n",
    "    y_draws = sts.nbinom.rvs(**kwargs_broadcast, size = y.shape + (n_draws_per_point,))\n",
    "    model_ps = sts.nbinom.pmf(k = y_draws, **kwargs_broadcast)\n",
    "    square_term = model_ps.mean()\n",
    "    \n",
    "    return square_term - 2 * cross_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "log_alpha_sizes = np.linspace(1.5, 7, 15)\n",
    "log_alpha_sizes = np.linspace(6, 10, 15)\n",
    "cv_scores = do_cv_nbinom(X, y, log_alpha_sizes = log_alpha_sizes, base_alphas = alpha_base, \n",
    "                         loglike_method = 'nb1', \n",
    "                         splitter = splitter,\n",
    "                         disp = False,\n",
    "                         trim_mode = 'off',\n",
    "                         maxiter = 200)#,\n",
    "                         #qc_tol = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('All Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].T, color = color)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Mean Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].mean(axis = 0), color = color)\n",
    "    \n",
    "plt.legend(cv_scores.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_alpha_sizes, cv_scores['test'].mean(axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cutoff = 8.5\n",
    "best_ind = (log_alpha_sizes < best_cutoff).sum() - 1\n",
    "best_log_alpha['derivative'] = log_alpha_sizes[best_ind]\n",
    "best_scores['derivative'] = {key : cv_scores[key][:, best_ind] for key in cv_scores}\n",
    "best_log_alpha['derivative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['derivative'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['derivative'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for Best Derivative Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Best Results For All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 1\n",
    "tick_labels = []\n",
    "for model in best_scores:\n",
    "    for key, scores in best_scores[model].items():\n",
    "        plt.boxplot(scores, positions = [position])\n",
    "        tick_labels.append(model + '_' + key)\n",
    "        position += 1\n",
    "plt.xticks(np.arange(1, len(tick_labels) + 1), tick_labels, rotation = 25)\n",
    "plt.ylabel('Log10 Likelihood')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "position = 1\n",
    "tick_labels = []\n",
    "for model in list(best_scores.keys())[1:]:\n",
    "    for key, scores in best_scores[model].items():\n",
    "        plt.boxplot(scores, positions = [position])\n",
    "        tick_labels.append(model + '_' + key)\n",
    "        position += 1\n",
    "plt.xticks(np.arange(1, len(tick_labels) + 1), tick_labels, rotation = 25)\n",
    "plt.ylabel('Log10 Likelihood')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha_size = np.exp(best_log_alpha['derivative'])\n",
    "final_negative_binomial = NegativeBinomial(endog = y,\n",
    "                                           exog = statsmodels.tools.add_constant(X, has_constant = 'add'),\n",
    "                                           loglike_method = 'nb1')\n",
    "alpha_const_base = np.concatenate([[0], alpha_base, [0]])\n",
    "start_params = get_nbinomial_start_params(X, y, loglike_method = 'nb1')\n",
    "final_result = final_negative_binomial.fit_regularized(alpha = best_alpha_size * alpha_const_base,\n",
    "                                                       max_iter = 300,\n",
    "                                                       start_params = start_params,\n",
    "                                                       trim_mode = 'off')#, acc = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['const', 'alpha']:\n",
    "    print('final_result.params[\\'' + key + '\\'] = ', \n",
    "          '{:.2f}'.format(final_result.params[key]))\n",
    "\n",
    "# Don't include constant offset in graph.\n",
    "plt.plot(np.arange(len(final_result.params))[1:-1], np.abs(final_result.params[1:-1]))\n",
    "plt.axvline(2 * n_daily_fourier, color = 'red')\n",
    "plt.show()\n",
    "\n",
    "plot_frequency = np.stack([all_frequency, all_frequency], axis = -1).reshape(-1)\n",
    "#plt.plot(all_frequency, final_result.params[1:-1:2])\n",
    "#plt.plot(all_frequency, final_result.params[2:-1:2])\n",
    "plt.plot(all_frequency, np.abs(final_result.params[1:-1:2]))\n",
    "plt.plot(all_frequency, np.abs(final_result.params[2:-1:2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbinom_intervals(fitted_model, X, percentage, loglike_method, add_constant = True):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      nb2:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)\n",
    "    \n",
    "    intervals = sts.nbinom.interval(percentage, **nbinom_kwargs)\n",
    "    print(intervals[1][:3])\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.scatter(day_of_week, hour_counts)\n",
    "labels = []\n",
    "percentile = 0.95\n",
    "\n",
    "p_days = np.linspace(0, 7, 7 * 24 * 3)\n",
    "plot_X = make_periodic(p_days, all_periods, return_alphas = None)\n",
    "plot_X = statsmodels.tools.add_constant(plot_X, has_constant = 'add')\n",
    "\n",
    "intervals = get_nbinom_intervals(final_result, plot_X, percentile, loglike_method = 'nb1', add_constant = False)\n",
    "means = final_result.predict(plot_X)\n",
    "plt.plot(p_days, means, color = 'red')\n",
    "for endpoint_i in range(2):\n",
    "    plt.plot(p_days, intervals[endpoint_i], color = 'purple')\n",
    "        \n",
    "plt.legend(['model mean', 'model ' + str(int(percentile *100)) + '% interval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uber_data",
   "language": "python",
   "name": "uber_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
