{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We aggregate the uber pickup data into hourly pick up counts (i.e. 24 samples per day). We look at the hourly pickup counts\n",
    "as a function of day of the week (i.e. time of the week as we allow \"days\" like 0.5 which would be Monday noon). Note that we use the convention that the week start with Monday at 0 and Sunday Midnight is 7. We use a periodic model so that our model\n",
    "transitions smoothly between the end of the week and the start of the week.\n",
    "\n",
    "Here is a graph of our final regularized model:\n",
    "\n",
    "![Graph of final model](files/graphs/final_model_counts.svg?santize=true)\n",
    "\n",
    "This model allows us to pick out intervals where 95% of all pickups lie for a given time of the week.\n",
    "\n",
    "# Model\n",
    "\n",
    "We model the data using a regularized negative binomial regression. This is a common model for dealing with data that should be\n",
    "poisson, but has variances that are too large. The idea is that the conditional distribution of `y = hourly pickup count`\n",
    "given a value `x = (continuous) day of the week` is a latent poisson mixture where the poisson mean of y is distributed by\n",
    "a latent gamma distribution. The resulting conditional distribution gives `y | x` a negative binomial distribution.\n",
    "\n",
    "Let us give a brief description of the model. Technical details may be found in\n",
    "[Greene *Functional Forms for the Negative Binomial Model for Count Data*](http://people.stern.nyu.edu/wgreene/EconomicsLetters-NBP.pdf).\n",
    "The idea is that there is a latent variable `g` that is distributed as a gamma distribution with mean 1 and variance to be\n",
    "determined. Given a value of `x` and `g`, `y` is distrbuted as a poisson distribution with mean `m(x) * g`, where `m(x)` is log-linear, i.e. `log m(x) = Ax + B`.\n",
    "\n",
    "There is more than one way to choose a variance for `g`. We will look at two methods:\n",
    "* `nb1` : The variance of `g` is chosen such that `Variance(y|x) = m(x) + alpha * m(x)` for some constant parameter `alpha`.\n",
    "* `nb2` : The variance of `g` is chosen such that `Varaince(y|x) = m(x) + alpha * m(x)**2` for some constant parameter `alpha`.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "We will look at using regularization weights that are:\n",
    "* constant for different fourier inputs.\n",
    "* are weighted according to the derivatives of the fourier inputs.\n",
    "\n",
    "Weighting the regularization penalties in a way related to the derivatives of the fourier terms is related to punishing the\n",
    "amount of \"wiggle\" introduced by the term.\n",
    "\n",
    "\n",
    "# Model Selection\n",
    "\n",
    "We need to choose a type of regularization weight and whether to use `nb1` or `nb2` negative binomial model.\n",
    "\n",
    "We use cross-validation where we score our hold-out set on its mean log-likelihood for the trained model. Here are the results:\n",
    "\n",
    "![All Model Scores](files/graphs/loglikelihood_all_models.svg?sanitize=true)\n",
    "\n",
    "We see that the unregularized model has significantly worse log-likelihood scores. Let us take a closer look at the regularized\n",
    "models:\n",
    "\n",
    "![Regularized Model Scores](files/graphs/loglikelihood_regularized_models.svg?sanitize=true)\n",
    "\n",
    "Based on the scores and the fact that a rough measure of correlation between graphing the log-mean vs log-variance for\n",
    "binned statistics, we choose to use the derivative-based regularization with `nb1` negative binomial model.\n",
    "\n",
    "# Final Model\n",
    "\n",
    "We train the final model on the entire dataset. The graph of the final result is above. We also get the\n",
    "following for the Fourier Coefficients sizes:\n",
    "\n",
    "![Model Coefficient Sizes](files/graphs/fourier_coeff_sizes.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from my_src import my_graphs, my_periodic\n",
    "\n",
    "# When saving svg plots, we use regular text for plot text. This stops the default\n",
    "# behavior of outputting curves for each letter of text and saves a lot of memory\n",
    "# per graph.\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "sns.set() # Let seaborn handle graphs.\n",
    "violin_plot_grid_size = 20\n",
    "violin_plot_fig_size = (12, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/uber-raw-data-aug14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date/Time'] = pd.to_datetime(data['Date/Time'])\n",
    "print(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_date = data['Date/Time'].apply(lambda x : (x.month, x.day, x.hour))\n",
    "reduced_date = pd.DataFrame(reduced_date.tolist(), columns = ['month', 'day', 'hour'])\n",
    "reduced_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get hourly counts by day and hour.\n",
    "\n",
    "hour_counts = (reduced_date.groupby(['day', 'hour'])\n",
    "                .count()\n",
    "                .rename(columns =  {'month' : 'count'}))\n",
    "hour_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_day = 4\n",
    "day_of_week = hour_counts.index.to_series().apply(lambda x : (x[0] - 1 + initial_day) % 7)\n",
    "print('Before adding in hourly part\\n', day_of_week.value_counts().sort_index())\n",
    "day_of_week += hour_counts.index.get_level_values('hour') / 24\n",
    "day_of_week.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(day_of_week, hour_counts['count'])\n",
    "plt.xlabel('Day of the Week (to the Hour)')\n",
    "plt.ylabel('Hourly Pickup Count')\n",
    "plt.title('Hourly Pickup Counts Through the Week')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/continuous_hourly_counts.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the hourly pickup counts for the different times of the week.\n",
    "\n",
    "![Hourly Pickup Counts vs Continuous Day of the Week](files/graphs/continuous_hourly_counts.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough Statistics Using Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bin statistics.\n",
    "\n",
    "bin_size_hours = 2\n",
    "bin_size = 1 / 24 * bin_size_hours\n",
    "bins = np.arange(0, 7 + 1/24, bin_size)\n",
    "bin_cut = pd.cut(day_of_week, bins = bins)\n",
    "bin_mean = hour_counts.groupby(bin_cut).mean().sort_index()\n",
    "bin_variance = (hour_counts.groupby(bin_cut).std()**2).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the bin variance vs the bin mean. Use logarithmic scales.\n",
    "\n",
    "bin_model = LinearRegression()#fit_intercept = False)\n",
    "bin_model.fit(np.log10(bin_mean), np.log10(bin_variance).values.reshape(-1))\n",
    "print('log_bin_variance = ', '{:.3f}'.format(bin_model.coef_[0]), ' * log_bin_mean + ',\n",
    "      '{:.3f}'.format(bin_model.intercept_))\n",
    "\n",
    "line_x = np.linspace(np.log10(bin_mean.min()), np.log10(bin_mean.max()), 4)\n",
    "plt.scatter(np.log10(bin_mean), np.log10(bin_variance))\n",
    "plt.plot(line_x, bin_model.predict(line_x), color = 'red')\n",
    "plt.xlabel('Log10(Bin Mean)')\n",
    "plt.ylabel('Log10(Bin Variance)')\n",
    "plt.title('Comparison of Bin Variances to Bin Means')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/bin_var_bin_mean.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the logarithms of the bin variances to the logarithms of the bin means, we see that\n",
    "```\n",
    "log_bin_variance = 1.251 * log_bin_mean + 0.352\n",
    "```\n",
    "\n",
    "![Log10(Bin_Variance) vs Log10(Bin_Mean)](files/graphs/bin_var_bin_mean.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Periodic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the periods for weekly and daily trends.\n",
    "\n",
    "n_weekly_fourier = 30\n",
    "n_daily_fourier = 11 # For daily fourier, don't go over 11; the data is sampled by the hour.\n",
    "                     # Remember that dimension doubles and can't exceed 24. Also we are\n",
    "                     # already adding constant functions in addition to fourier stuff.\n",
    "\n",
    "weekly_frequency = np.arange(1, n_weekly_fourier + 1) # per week\n",
    "daily_frequency = 7 * np.arange(1, n_daily_fourier + 1) # per week\n",
    "all_frequency = np.concatenate([weekly_frequency, daily_frequency])\n",
    "all_frequency = np.unique(all_frequency)\n",
    "\n",
    "all_periods = 7.0 / all_frequency\n",
    "print(all_periods.shape)\n",
    "all_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the periodic features and the alphas for regularization.\n",
    "\n",
    "y = hour_counts['count']\n",
    "X, alpha_base = my_periodic.make_periodic(day_of_week, all_periods, return_alphas = 'L1')\n",
    "\n",
    "column_names = ['freq_' + str(freq) for freq in all_frequency]\n",
    "column_names = [[col + '_c', col + '_s'] for col in column_names]\n",
    "column_names = np.array(column_names).reshape(-1)\n",
    "X = pd.DataFrame(X, index = day_of_week.index, columns = column_names)\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Non-Regularized NegativeBinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = {'unregularized' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_nbinom_params(statsmodels_params, means, loglike_method):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      nb2:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    \n",
    "    theta = 1 / statsmodels_params['alpha']\n",
    "    if loglike_method == 'nb2':\n",
    "        ns = theta\n",
    "        ps = 1 / (1 + statsmodels_params['alpha'] * means)\n",
    "    elif loglike_method == 'nb1':\n",
    "        ns = theta * means\n",
    "        ps = theta / (1 + theta)\n",
    "    else:\n",
    "        raise Exception('loglike_method not a valid value.')\n",
    "    return {'n' : ns, 'p' : ps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function for computing log-likelihood of negative binomial model.\n",
    "\n",
    "def find_log10_likelihood(fitted_model, X, y, loglike_method, add_constant = True):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)   \n",
    "        \n",
    "    log_lls = sts.nbinom.logpmf(k = y, **nbinom_kwargs) / np.log(10)\n",
    "    return log_lls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbinomial_start_params(X, y, loglike_method, add_constant = True):\n",
    "    _, n_zeros = X.shape\n",
    "    if add_constant:\n",
    "        n_zeros = n_zeros + 1\n",
    "        \n",
    "    y_mean = y.mean()\n",
    "    if loglike_method == 'nb2':\n",
    "        initial_alpha = (y.std()**2 - y_mean) / y_mean**2\n",
    "    elif loglike_method == 'nb1':\n",
    "        initial_alpha = (y.std()**2 - y_mean) / y_mean\n",
    "    elif loglike_method != 'geometric':\n",
    "        raise Exception('loglike_method value not recognized.')\n",
    "        \n",
    "    if loglike_method != 'geometric':\n",
    "        start_params = np.zeros(n_zeros + 1)\n",
    "        start_params[-1] = initial_alpha\n",
    "    else:\n",
    "        start_params = np.zeros(n_zeros)\n",
    "        \n",
    "    start_params[0] = np.log(y_mean)\n",
    "    return start_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbinom_intervals(fitted_model, X, percentage, loglike_method, add_constant = True):\n",
    "    # statsmodels negative binomial fit results seems to have no implementation for\n",
    "    # finding the log-likelihood of samples in X and y. So we will use the scipy.stats\n",
    "    # implementation of the negative binomial distribution to find the log-likelihood.\n",
    "    # Translation between two libraries:\n",
    "    #      nb2:\n",
    "    #      scipy.stats.nbinom n = statsmodels alpha\n",
    "    #      scipy.stats.nbinom p = statsmodels theta / (theta + mean(X))\n",
    "    #                           = 1 / (1 + alpha * mean(X))\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)\n",
    "    \n",
    "    intervals = sts.nbinom.interval(percentage, **nbinom_kwargs)\n",
    "    return intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "start_params = get_nbinomial_start_params(X, y, loglike_method = 'nb1')\n",
    "\n",
    "best_scores['unregularized'] = {key : [] for key in ['train', 'test']}\n",
    "\n",
    "for split_ind in splitter.split(X):\n",
    "    keys = ['train', 'test']\n",
    "    X_split = {key : statsmodels.tools.add_constant(X.loc[ind, :], has_constant = 'add') for key, ind in zip(keys, split_ind)}\n",
    "    y_split = {key : y.loc[ind, :] for key, ind in zip(keys, split_ind)}\n",
    "    negative_binomial = NegativeBinomial(endog = y_split['train'], exog = X_split['train'], loglike_method = 'nb1')\n",
    "    fitted_model = negative_binomial.fit(start_params = start_params)\n",
    "    \n",
    "    for key in best_scores['unregularized']:\n",
    "            log_ll = find_log10_likelihood(fitted_model, X_split[key], y_split[key],\n",
    "                                           loglike_method = 'nb1').mean()\n",
    "            best_scores['unregularized'][key].append(log_ll)\n",
    "            \n",
    "for key in best_scores['unregularized']:\n",
    "    best_scores['unregularized'][key] = np.array(best_scores['unregularized'][key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['unregularized'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['unregularized'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for UnRegularized Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.scatter(day_of_week, hour_counts)\n",
    "labels = []\n",
    "percentile = 0.95\n",
    "\n",
    "p_days = np.linspace(0, 7, 7 * 24 * 3)\n",
    "plot_X = my_periodic.make_periodic(p_days, all_periods, return_alphas = None)\n",
    "plot_X = statsmodels.tools.add_constant(plot_X, has_constant = 'add')\n",
    "\n",
    "intervals = get_nbinom_intervals(fitted_model, plot_X, percentile, loglike_method = 'nb1', add_constant = False)\n",
    "means = fitted_model.predict(plot_X)\n",
    "plt.plot(p_days, means, color = 'red')\n",
    "for endpoint_i in range(2):\n",
    "    plt.plot(p_days, intervals[endpoint_i], color = 'purple')\n",
    "        \n",
    "plt.legend(['model mean', 'model ' + str(int(percentile *100)) + '% interval'])\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Hourly Pick Up Count')\n",
    "plt.title('Unregularized Model Hourly Pickup Counts')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/unregularized_model_counts.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the graph of one of the unregularized models.\n",
    "\n",
    "![Unregularized Model](files/graphs/unregularized_model_counts.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Function for Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gridsearch_nbinom(X, y, alphas, loglike_method, **fit_kwargs):\n",
    "    negative_binomial = NegativeBinomial(endog = y['train'],\n",
    "                                         exog = X['train'],\n",
    "                                         loglike_method = loglike_method)\n",
    "    search_scores = {key : [] for key in X}\n",
    "    start_params = get_nbinomial_start_params(X['train'], y['train'], loglike_method, add_constant = False)\n",
    "    \n",
    "    for alpha in alphas: \n",
    "        fitted_model = negative_binomial.fit_regularized(alpha = alpha, \n",
    "                                                         start_params = start_params,\n",
    "                                                         **fit_kwargs)\n",
    "        for key in search_scores:\n",
    "            log_ll = find_log10_likelihood(fitted_model, X[key], y[key],\n",
    "                                           loglike_method = loglike_method).mean()\n",
    "            search_scores[key].append(log_ll)\n",
    "        # Reset start_params to last fitted values for speed-up.\n",
    "        start_params = np.array(fitted_model.params)\n",
    "        \n",
    "    return search_scores\n",
    "    \n",
    "def do_cv_nbinom(X, y, log_alpha_sizes, base_alphas, loglike_method, splitter, add_constant = True, **fit_kwargs):\n",
    "    if add_constant:\n",
    "        base_alphas = np.concatenate([[0], # No penalty for bias term.\n",
    "                                      base_alphas, # Penalties for coefficients.\n",
    "                                      [0]]) # No penalty for alpha parameter related to latent\n",
    "                                            # Poisson distribution.\n",
    "        X = statsmodels.tools.add_constant(X, has_constant = 'add')\n",
    "    else:\n",
    "        base_alphas = np.concatenate([base_alphas, # Penalties for coefficients.\n",
    "                                     [0]]) # No penalty for alpha parameter related to latent\n",
    "                                           # Poisson distribution.\n",
    "    cv_scores = {'train' : [], 'test' : []}   \n",
    "    \n",
    "    for split_ind in splitter.split(X, y):\n",
    "        split_ind = {'train' : X.index[split_ind[0]],\n",
    "                     'test' : X.index[split_ind[1]]}\n",
    "        X_split = {key : X.loc[ind] for key, ind in split_ind.items()} \n",
    "        y_split = {key : y.loc[ind] for key, ind in split_ind.items()}\n",
    "        alphas = (base_alphas * np.exp(log_size) for log_size in log_alpha_sizes)\n",
    "        search_scores = do_gridsearch_nbinom(X_split, y_split, alphas, loglike_method, **fit_kwargs)\n",
    "        for key in cv_scores:\n",
    "            cv_scores[key].append(search_scores[key])\n",
    "        \n",
    "    for key in cv_scores:\n",
    "        cv_scores[key] = np.array(cv_scores[key])\n",
    "    return cv_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Regularized Fit With Flat Penalty Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_alpha = {'flat' : None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "log_alpha_sizes = np.linspace(6, 8, 15)\n",
    "flat_alpha_base = np.full(alpha_base.shape, 1.0)\n",
    "flat_alpha_base = flat_alpha_base / np.linalg.norm(flat_alpha_base)\n",
    "cv_scores = do_cv_nbinom(X, y, log_alpha_sizes = log_alpha_sizes, base_alphas = flat_alpha_base, \n",
    "                         loglike_method = 'nb1', \n",
    "                         splitter = splitter,\n",
    "                         trim_mode = 'off',\n",
    "                         maxiter = 300,\n",
    "                         disp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('All Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].T, color = color)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Mean Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].mean(axis = 0), color = color)\n",
    "    \n",
    "plt.legend(cv_scores.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_alpha_sizes, cv_scores['test'].mean(axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cutoff = 7.25\n",
    "best_ind = (log_alpha_sizes < best_cutoff).sum() - 1\n",
    "best_log_alpha['flat'] = log_alpha_sizes[best_ind]\n",
    "best_scores['flat'] = {key : cv_scores[key][:, best_ind] for key in cv_scores}\n",
    "best_log_alpha['flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['flat'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['flat'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for Best Flat Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Regularized Fit With Regularization Depending on Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to estimate risk between model distribution and real distribution.\n",
    "# Note we don't calculute the square term for the true distribution as it is\n",
    "# independent of the model.\n",
    "\n",
    "def find_risk(fitted_model, X, y, loglike_method, add_constant = True, n_draws_per_point = 50):\n",
    "    if add_constant:\n",
    "        X = statsmodels.tools.add_constant(X)\n",
    "    means = fitted_model.predict(X)\n",
    "    nbinom_kwargs = translate_nbinom_params(fitted_model.params, means, loglike_method)\n",
    "    \n",
    "    # Approximate the cross-term by turning integral in over true distribution into a sample\n",
    "    # mean of model probability.\n",
    "    \n",
    "    cross_term = sts.nbinom.pmf(k = y, **nbinom_kwargs).mean()\n",
    "    \n",
    "    # Approximate the square-term by making draws from the model distribution for each X_i.\n",
    "    kwargs_broadcast = {key : np.array(param).reshape(-1, 1) for key, param in nbinom_kwargs.items()}\n",
    "    y_draws = sts.nbinom.rvs(**kwargs_broadcast, size = y.shape + (n_draws_per_point,))\n",
    "    model_ps = sts.nbinom.pmf(k = y_draws, **kwargs_broadcast)\n",
    "    square_term = model_ps.mean()\n",
    "    \n",
    "    return square_term - 2 * cross_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBD1\n",
    "\n",
    "Here we look at when the `variance = mean + alpha * mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "log_alpha_sizes = np.linspace(1.5, 7, 15)\n",
    "log_alpha_sizes = np.linspace(6, 10, 15)\n",
    "cv_scores = do_cv_nbinom(X, y, log_alpha_sizes = log_alpha_sizes, base_alphas = alpha_base, \n",
    "                         loglike_method = 'nb1', \n",
    "                         splitter = splitter,\n",
    "                         disp = False,\n",
    "                         trim_mode = 'off',\n",
    "                         maxiter = 200)#,\n",
    "                         #qc_tol = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('All Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].T, color = color)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Mean Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].mean(axis = 0), color = color)\n",
    "    \n",
    "plt.legend(cv_scores.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_alpha_sizes, cv_scores['test'].mean(axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cutoff = 8.5\n",
    "best_ind = (log_alpha_sizes < best_cutoff).sum() - 1\n",
    "best_log_alpha['derivative_nbd1'] = log_alpha_sizes[best_ind]\n",
    "best_scores['derivative_nbd1'] = {key : cv_scores[key][:, best_ind] for key in cv_scores}\n",
    "best_log_alpha['derivative_nbd1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['derivative_nbd1'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['derivative_nbd1'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for Best Derivative Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBD2\n",
    "\n",
    "Try model with `variance = mean + alpha * mean**2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = KFold(n_splits = 5, shuffle = True)\n",
    "log_alpha_sizes = np.linspace(1.5, 7, 15)\n",
    "log_alpha_sizes = np.linspace(6, 10, 15)\n",
    "cv_scores = do_cv_nbinom(X, y, log_alpha_sizes = log_alpha_sizes, base_alphas = alpha_base, \n",
    "                         loglike_method = 'nb2', \n",
    "                         splitter = splitter,\n",
    "                         disp = False,\n",
    "                         trim_mode = 'off',\n",
    "                         maxiter = 200)#,\n",
    "                         #qc_tol = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('All Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].T, color = color)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Mean Scores')\n",
    "for key, color in zip(cv_scores.keys(), ['blue', 'red']):\n",
    "    plt.plot(log_alpha_sizes, cv_scores[key].mean(axis = 0), color = color)\n",
    "    \n",
    "plt.legend(cv_scores.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.plot(log_alpha_sizes, cv_scores['test'].mean(axis = 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cutoff = 8.5\n",
    "best_ind = (log_alpha_sizes < best_cutoff).sum() - 1\n",
    "best_log_alpha['derivative_nbd2'] = log_alpha_sizes[best_ind]\n",
    "best_scores['derivative_nbd2'] = {key : cv_scores[key][:, best_ind] for key in cv_scores}\n",
    "best_log_alpha['derivative_nbd2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for maximum score.\n",
    "\n",
    "for position, scores in enumerate(best_scores['derivative_nbd2'].values()):\n",
    "    plt.boxplot(scores, positions = [position])\n",
    "\n",
    "plt.xticks([0, 1], best_scores['derivative_nbd2'].keys())\n",
    "plt.ylabel('Mean Log10 Likelihood')\n",
    "plt.title('Log10 Likelihoods for Best Derivative Fit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Best Results For All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 5))\n",
    "position = 1\n",
    "tick_labels = []\n",
    "for model in best_scores:\n",
    "    for key, scores in best_scores[model].items():\n",
    "        plt.boxplot(scores, positions = [position])\n",
    "        tick_labels.append(model + '_' + key)\n",
    "        position += 1\n",
    "plt.xticks(np.arange(1, len(tick_labels) + 1), tick_labels, rotation = 25)\n",
    "plt.ylabel('Log10 Likelihood')\n",
    "plt.title('Log10 Likelihood For Different Methods')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/loglikelihood_all_models.svg')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "position = 1\n",
    "tick_labels = []\n",
    "for model in list(best_scores.keys())[1:]:\n",
    "    for key, scores in best_scores[model].items():\n",
    "        plt.boxplot(scores, positions = [position])\n",
    "        tick_labels.append(model + '_' + key)\n",
    "        position += 1\n",
    "plt.xticks(np.arange(1, len(tick_labels) + 1), tick_labels, rotation = 25)\n",
    "plt.ylabel('Log10 Likelihood')\n",
    "plt.title('Log10 Likelihood For Different Regularized Methods')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/loglikelihood_regularized_models.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results for the different choices of models. First let's look at all of the models:\n",
    "\n",
    "![Log10 Likelihood of All Models](files/graphs/loglikelihood_all_models.svg?sanitize=true)\n",
    "\n",
    "We see that the regularized models perform much better than the non-regularized model.\n",
    "\n",
    "![Log10 Likelihood of Regularized Models](files/graphs/loglikelihood_regularized_models.svg?sanitize=true)\n",
    "\n",
    "There isn't considerably much difference in performance for the different regularized models. However, the `nbd1` model\n",
    "seems like it should be a better fit than the `nbd2` model based on the correlations between the binned means and binned variances. Also, it seems intuitive that using regularization parameters depending on the derivatives makes more sense than\n",
    "just flat regularization parameters. So we will use the regularized weights related to the derivatives of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final model and do a fit to all of the data.\n",
    "\n",
    "best_alpha_size = np.exp(best_log_alpha['derivative_nbd1'])\n",
    "final_negative_binomial = NegativeBinomial(endog = y,\n",
    "                                           exog = statsmodels.tools.add_constant(X, has_constant = 'add'),\n",
    "                                           loglike_method = 'nb1')\n",
    "alpha_const_base = np.concatenate([[0], alpha_base, [0]])\n",
    "start_params = get_nbinomial_start_params(X, y, loglike_method = 'nb1')\n",
    "final_result = final_negative_binomial.fit_regularized(alpha = best_alpha_size * alpha_const_base,\n",
    "                                                       max_iter = 300,\n",
    "                                                       start_params = start_params,\n",
    "                                                       trim_mode = 'off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph the coefficients of the final model.\n",
    "\n",
    "for key in ['const', 'alpha']:\n",
    "    print('final_result.params[\\'' + key + '\\'] = ', \n",
    "          '{:.2f}'.format(final_result.params[key]))\n",
    "\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "plot_frequency = np.stack([all_frequency, all_frequency], axis = -1).reshape(-1)\n",
    "plt.plot(all_frequency, np.abs(final_result.params[1:-1:2]))\n",
    "plt.plot(all_frequency, np.abs(final_result.params[2:-1:2]))\n",
    "plt.scatter(all_frequency, np.abs(final_result.params[1:-1:2]))\n",
    "plt.scatter(all_frequency, np.abs(final_result.params[2:-1:2]))\n",
    "plt.xlabel('Frequency (per week)')\n",
    "plt.legend(['Cosine', 'Sine'])\n",
    "plt.ylabel('Size')\n",
    "plt.title('Sizes of Fourier Coefficients for Final Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/fourier_coeff_sizes.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the sizes of the coefficients of the final model.\n",
    "\n",
    "![Fourier Coefficient Sizes](files/graphs/fourier_coeff_sizes.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 5))\n",
    "plt.scatter(day_of_week, hour_counts)\n",
    "labels = []\n",
    "percentile = 0.95\n",
    "\n",
    "p_days = np.linspace(0, 7, 7 * 24 * 3)\n",
    "plot_X = my_periodic.make_periodic(p_days, all_periods, return_alphas = None)\n",
    "plot_X = statsmodels.tools.add_constant(plot_X, has_constant = 'add')\n",
    "\n",
    "intervals = get_nbinom_intervals(final_result, plot_X, percentile, loglike_method = 'nb1', add_constant = False)\n",
    "means = final_result.predict(plot_X)\n",
    "plt.plot(p_days, means, color = 'red')\n",
    "for endpoint_i in range(2):\n",
    "    plt.plot(p_days, intervals[endpoint_i], color = 'purple')\n",
    "        \n",
    "plt.legend(['model mean', 'model ' + str(int(percentile *100)) + '% interval'])\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Hourly Pick Up Count')\n",
    "plt.title('Final Model Hourly Pickup Counts')\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/final_model_counts.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the final model matches the data and get the interval for where 95% of all counts lie (depending\n",
    "on the day of the week).\n",
    "\n",
    "![Final Model Hourly Pickup Counts](files/graphs/final_model_counts.svg?sanitize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uber_data",
   "language": "python",
   "name": "uber_data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
